{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn \n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_excel('./DS_Assignment_2/height_and_pose.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Depthmap Image</th>\n",
       "      <th>Height(cm)</th>\n",
       "      <th>Pose</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1d5fee00-ada4-11eb-a80b-3f50af21830f</td>\n",
       "      <td>76.7</td>\n",
       "      <td>[{'bbox_coordinates': [(158.91087, 92.53168), ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1eddf4c0-ad9f-11eb-a80b-ef4adba049aa</td>\n",
       "      <td>66.4</td>\n",
       "      <td>[{'bbox_coordinates': [(131.00702, 68.115364),...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>38e29bc0-8fc0-11eb-b5f2-0742da91b282</td>\n",
       "      <td>63.2</td>\n",
       "      <td>[{'bbox_coordinates': [(116.070114, 37.567608)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6cd5d480-8b8b-11eb-b152-f7b115384fa9</td>\n",
       "      <td>87.1</td>\n",
       "      <td>[{'bbox_coordinates': [(145.24242, 26.49348), ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>76ed6fc0-8e1d-11eb-ad70-7be1a1219834</td>\n",
       "      <td>59.1</td>\n",
       "      <td>[{'bbox_coordinates': [(150.17046, 65.00379), ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Depthmap Image  Height(cm)  \\\n",
       "0  1d5fee00-ada4-11eb-a80b-3f50af21830f        76.7   \n",
       "1  1eddf4c0-ad9f-11eb-a80b-ef4adba049aa        66.4   \n",
       "2  38e29bc0-8fc0-11eb-b5f2-0742da91b282        63.2   \n",
       "3  6cd5d480-8b8b-11eb-b152-f7b115384fa9        87.1   \n",
       "4  76ed6fc0-8e1d-11eb-ad70-7be1a1219834        59.1   \n",
       "\n",
       "                                                Pose  \n",
       "0  [{'bbox_coordinates': [(158.91087, 92.53168), ...  \n",
       "1  [{'bbox_coordinates': [(131.00702, 68.115364),...  \n",
       "2  [{'bbox_coordinates': [(116.070114, 37.567608)...  \n",
       "3  [{'bbox_coordinates': [(145.24242, 26.49348), ...  \n",
       "4  [{'bbox_coordinates': [(150.17046, 65.00379), ...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def changeshape(layer):\n",
    "    x = F.relu(F.max_pool2d(F.conv2d(layer), kernel_size=2, stride=2))  # reduce spatial dimensions to (112, 112)\n",
    "    x = F.relu(F.max_pool2d(F.conv2d(layer), kernel_size=2, stride=2)) \n",
    "    downsampled_tensor = x\n",
    "    resized_layer = F.interpolate(downsampled_tensor, size=(54,54,1), mode='bilinear')\n",
    "    return resized_layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imageread(path,pose):\n",
    "    # img=Image.open(path)\n",
    "    # img = img.resize((224, 224), resample=Image.BILINEAR)\n",
    "    img=cv2.imread(path)\n",
    "    img=cv2.resize(img,(224,224))\n",
    "    st='bbox_confidence_score'\n",
    "    layer=torch.zeros_like(torch.tensor(img[:,:,0]))\n",
    "    pose=pose[1:-1]\n",
    "    pose=pose.replace('\\'','\\\"')\n",
    "    pose=pose.replace(\" \",\"\")\n",
    "    p=0\n",
    "    while(st!=pose[p:p+21]):\n",
    "        p=p+1\n",
    "    pose='{'+pose[p-1:]\n",
    "    # print(pose[0])\n",
    "    pose=json.loads(pose)\n",
    "    for i1,i2 in zip(pose['key_points_coordinate'],pose['key_points_prob']):\n",
    "        for j1,j2 in zip(i1,i2):\n",
    "            x=int((i1[j1]['x']/480)*224)\n",
    "            y=int((i1[j1]['y']/640)*224)\n",
    "            prob=i2[j2]['score']\n",
    "            # print(prob)\n",
    "            layer[x][y]=int((1-prob)*255.0)\n",
    "    return img,layer     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createModel(printtoggle=False):\n",
    "\n",
    "  class model(nn.Module):\n",
    "    def __init__(self,printtoggle):\n",
    "      super().__init__()\n",
    "\n",
    "      ### convolution layers\n",
    "      self.conv1 = nn.Conv2d(4,32,kernel_size=3,dtype=torch.float32)\n",
    "      self.conv1l = nn.Conv2d(1,1,kernel_size=3,dtype=torch.float32)\n",
    "\n",
    "      self.conv2 = nn.Conv2d(32,63,kernel_size=4,dtype=torch.float32)\n",
    "      self.conv2l = nn.Conv2d(1,1,kernel_size=4,dtype=torch.float32)\n",
    "\n",
    "      self.conv3 = nn.Conv2d(64,64,kernel_size=3,dtype=torch.float32)\n",
    "      # self.conv3l = nn.Conv2d(1,1,kernel_size=3,dtype=torch.float32)\n",
    "\n",
    "      self.conv4 = nn.Conv2d(64,64,kernel_size=3,dtype=torch.float32)\n",
    "      # self.conv4l = nn.Conv2d(1,1,kernel_size=3,dtype=torch.float32)\n",
    "\n",
    "      self.fc1 = nn.Linear(9216,512)\n",
    "\n",
    "      self.fc2 = nn.Linear(512,64)\n",
    "\n",
    "      self.out = nn.Linear(64,1)\n",
    "      self.print = printtoggle\n",
    "    # forward pass\n",
    "    def forward(self,x):\n",
    "      x=x.float()\n",
    "      layer=x[:,3,:,:] \n",
    "      print(f'Input: {x.shape}') if self.print else None\n",
    "\n",
    "      \n",
    "      x = F.relu(F.max_pool2d(self.conv1(x),2))\n",
    "      layer=F.relu(F.max_pool2d(self.conv1l(layer),2))\n",
    "      print(f'Layer conv1/pool1: {x.shape}') if self.print else None\n",
    "\n",
    "      \n",
    "      x = F.relu(F.max_pool2d(self.conv2(x),2))\n",
    "      layer=F.relu(F.max_pool2d(self.conv1l(layer),2))\n",
    "      print(f'Layer conv2/pool2: {x.shape}') if self.print else None\n",
    "      y=torch.zeros((x.shape[0],64,x.shape[2],x.shape[3]))\n",
    "      for i in range(63):\n",
    "        y[:,i,:,:]=x[:,i,:,:]  \n",
    "\n",
    "      y[:,63,:,:]=layer\n",
    "      x=y\n",
    "      x = F.relu(F.max_pool2d(self.conv3(x),2))\n",
    "\n",
    "      x = F.relu(F.max_pool2d(self.conv4(x),2))\n",
    "\n",
    "      nUnits = x.shape.numel()\n",
    "      x=x.view(int(nUnits))\n",
    "      if self.print: print(f'Vectorize: {x.shape}')\n",
    "      \n",
    "      x = F.relu(self.fc1(x))\n",
    "      if self.print: print(f'Layer fc1: {x.shape}')\n",
    "      x = F.relu(self.fc2(x))\n",
    "      if self.print: print(f'Layer fc2: {x.shape}')\n",
    "      x = self.out(x)\n",
    "      if self.print: print(f'Layer out: {x.shape}')\n",
    "\n",
    "      return x\n",
    "  \n",
    "  net = model(printtoggle)\n",
    "  \n",
    "  lossfun = nn.MSELoss()\n",
    "\n",
    "  optimizer = torch.optim.Adam(net.parameters(),lr=.001)\n",
    "\n",
    "  return net,lossfun,optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "net,lossfun,optimizer = createModel(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "img=cv2.imread('./DS_Assignment_2/depthmap/1d5fee00-ada4-11eb-a80b-3f50af21830f.jpg')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "img=cv2.resize(img,(224,224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "img,layer=imageread('./DS_Assignment_2/depthmap/1d5fee00-ada4-11eb-a80b-3f50af21830f.jpg',df.iloc[0]['Pose'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def datapreprocess(path,pose):\n",
    "    img,layer=imageread(path,pose)\n",
    "    img=torch.tensor(img,dtype=torch.float16)\n",
    "    img=img/255.0\n",
    "    new_img=torch.zeros(img.shape[0],img.shape[1],4)\n",
    "    # print(img.shape,' ',new_img.shape)\n",
    "    new_img[:,:,0]=img[:,:,0]\n",
    "    new_img[:,:,1]=img[:,:,1]\n",
    "    new_img[:,:,2]=img[:,:,2]\n",
    "    new_img[:,:,3]=layer\n",
    "    new_img=torch.tensor(new_img/255.0,dtype=torch.float16)\n",
    "    return new_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/s2/b_s__vrj425fzynlpm1vql3w0000gn/T/ipykernel_39819/3928052728.py:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  new_img=torch.tensor(new_img/255.0,dtype=torch.float16)\n"
     ]
    }
   ],
   "source": [
    "img_tensor=datapreprocess('./DS_Assignment_2/depthmap/1d5fee00-ada4-11eb-a80b-3f50af21830f.jpg',df.iloc[0]['Pose'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: torch.Size([1, 4, 224, 224])\n",
      "Layer conv1/pool1: torch.Size([1, 32, 111, 111])\n",
      "Layer conv2/pool2: torch.Size([1, 63, 54, 54])\n",
      "Vectorize: torch.Size([9216])\n",
      "Layer fc1: torch.Size([512])\n",
      "Layer fc2: torch.Size([64])\n",
      "Layer out: torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "yHat = net(img_tensor[None,:].permute(0, 3, 1, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def function2trainTheModel():\n",
    "  numepochs = 10\n",
    "  \n",
    "  # create a new model\n",
    "  net,lossfun,optimizer = createModel()\n",
    "\n",
    "  losses= torch.zeros(numepochs)\n",
    "\n",
    "  for epochi in range(numepochs):\n",
    "\n",
    "    net.train()\n",
    "    for X,z,y in zip(df['Depthmap Image'],df['Pose'],df['Height(cm)']):\n",
    "      img=datapreprocess('./DS_Assignment_2/depthmap/'+X+'.jpg',z)\n",
    "      # forward pass and loss\n",
    "      yHat = net(img[None,:].permute(0, 3, 1, 2))\n",
    "      loss = lossfun(yHat,torch.tensor([y]))\n",
    "\n",
    "      # backprop\n",
    "      optimizer.zero_grad()\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "\n",
    "      losses[epochi]=loss\n",
    "\n",
    "    # test accuracy\n",
    "    net.eval()\n",
    "\n",
    "    with torch.no_grad(): # deactivates autograd\n",
    "      yHat = net(img[None,:].permute(0, 3, 1, 2))\n",
    "\n",
    "  return losses,net\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/s2/b_s__vrj425fzynlpm1vql3w0000gn/T/ipykernel_39819/3928052728.py:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  new_img=torch.tensor(new_img/255.0,dtype=torch.float16)\n"
     ]
    }
   ],
   "source": [
    "losses,net = function2trainTheModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9 (pytorch)",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
